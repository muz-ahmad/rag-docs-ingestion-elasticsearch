{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Ingest Files From Cloud Object Storage to Databases for Elasticsearch\n\nThis notebook handles file ingestion from a bucket in Cloud Object Storage instance to a Databases for Elasticsearch Platinum Plan instance using code. Supported file types in the bucket include `pdf`, `docx`, `pptx`, `html`, and `txt`.\n\n**Prerequesites:**\n1. Sucessfully set up a connection to your Cloud Object Storage instance in the project\n2. Succesfully set up a connection to your Databases for Elasticsearch instance in the project\n3. Created an IBM Cloud API key in https://cloud.ibm.com/iam/apikeys and added it to your `Notebook_and_Deployment_Parameters` parameter set in the `ibm_cloud_apikey` field", "metadata": {}}, {"cell_type": "markdown", "source": "**Before running any cells, connect the notebook to the rest of the project by inserting a project token.**\n\nYou can do this by clicking the three vertical dots next to the `Code Snippet` icon on the top right of the notebook UI and then clicking `Insert project token` to insert the token at the beginning of your notebook. Make sure to execute the new cell at the top of the notebook before running any other notebook cells.", "metadata": {}}, {"cell_type": "markdown", "source": "## Configure Notebook\n\nThe cells below configure the notebook by importing the necessary packages and adding the values configured in your project's Parameter Sets to the namespace. The following values within the `Notebook_and_Deployment_Parameters` parameter set will affect the output of running this notebook\n\n| Parameter Set Variable Name | Description |\n| --- | --- |\n| `ingestion_chunk_overlap` | The number of overlapping tokens between each document. |\n| `ingestion_chunk_size` | The maximum number of tokens each document will contain. |\n| `es_index_name` | The name of the Elasticsearch index where the ingested data will be stored. |\n| `es_index_text_field` | The name of the field that will store your document text in the Elasticsearch index. |\n| `es_model_name` | The name of the model in Elasticsearch that will be used for further processing or querying. |", "metadata": {}}, {"cell_type": "code", "source": "import json\nimport time\nimport warnings\nimport nest_asyncio\nimport nltk\nimport elasticsearch.exceptions\nimport elasticsearch.helpers\nimport elastic_transport\nimport utils\n\nfrom llama_index.core import VectorStoreIndex, StorageContext, Settings\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.readers.file import (\n    PDFReader,\n    DocxReader,\n    UnstructuredReader,\n    FlatReader,\n    HTMLTagReader,\n)\nfrom llama_index.vector_stores.elasticsearch import ElasticsearchStore\nfrom ibm_watson_studio_lib import access_project_or_space\nfrom typing import Optional, Tuple\nfrom elasticsearch import Elasticsearch, AsyncElasticsearch\n\n# Credential settings\n# Here we are giving access to assets located in our watsonx.ai project\nwslib = access_project_or_space({\n        'token': '<YOUR WATSONX.AI PROJECT ACCESS TOKEN HERE>',\n        'project_id': '<YOUR WATSONX.AI PROJECT ID HERE>'\n})\n\nwslib.download_file(\"utils.py\")\n\nnest_asyncio.apply()\nnltk.download(\"averaged_perceptron_tagger\")\nwarnings.filterwarnings(\"ignore\")", "metadata": {"id": "d210d82d-b642-469d-b368-53e4e4a4756c"}, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/wsuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n", "name": "stderr"}], "execution_count": 4}, {"cell_type": "code", "source": "wslib_extension = utils.WSLibExtension(wslib)\nparams = wslib_extension.get_all_parameter_set_values()\nINGESTION_CHUNK_OVERLAP = params[\"ingestion_chunk_overlap\"]\nINGESTION_CHUNK_SIZE = params[\"ingestion_chunk_size\"]\nINDEX_NAME = params[\"es_index_name\"]\nINDEX_TEXT_FIELD = params[\"es_index_text_field\"]\nEMBEDDING_MODEL_NAME = params[\"es_model_name\"]\nDEFAULT_READERS = {\n    \".pdf\": PDFReader(),\n    \".docx\": DocxReader(),\n    \".pptx\": UnstructuredReader(),\n    \".txt\": FlatReader(),\n    \".html\": HTMLTagReader(),\n}", "metadata": {"id": "ab3bf7fd-797c-43b3-b2b4-ba6898217625"}, "outputs": [], "execution_count": 5}, {"cell_type": "markdown", "source": "## Read and Prepare Files From Cloud Object Storage\n\nThe cells below connect to your Cloud Object Storage bucket. Then, the files inside the bucket are read, chunked, and formatted into JSON objects which can be ingested by Watsonx Discovery. The cells use the LlamaIndex framework and [LlamaIndex file readers](https://llamahub.ai/l/readers/llama-index-readers-file?from=all) to perform the file ingestion. If you want to use another file reader for a particular extension, import the desired reader and edit the `DEFAULT_READERS` dictionary with the reader you want to use. The default schema for the ingested JSON objects is described below for reference. In addition to the schema below, this notebook also adds `url` and `file_name` to each document to ease the integration with the Watsonx Assistant native search extension.\n\n| Level 1 Key | Level 2 Key | Level 3 Key | Level 4 Key | Description |\n| --- | --- | --- | --- | --- |\n| `_id` | | | | The unique identifier of the document. |\n| `_source` | | | | Contains the main data of the document. |\n| `_source` | `hash` | | | The hash of the document. |\n| `_source` | `metadata` | | | Contains metadata about the document. |\n| `_source` | `metadata` | `file_name` | | The name of the file from which the document was created. |\n| `_source` | `metadata` | `page_label` | | The label of the page from which the document was created. |\n| `_source` | `relationships` | | | Contains relationships of the document with other documents. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | | Contains information about the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `hash` | The hash of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `metadata` | Metadata of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `node_id` | The unique identifier of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `node_type` | The type of the next document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | | Contains information about the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `hash` | The hash of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `metadata` | Metadata of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `node_id` | The unique identifier of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `node_type` | The type of the source document. |\n| `_source` | `text_field` | | | The text content of the document. |", "metadata": {}}, {"cell_type": "markdown", "source": "### Connect to Cloud Object Storage", "metadata": {}}, {"cell_type": "code", "source": "cos_connection_dict = wslib.get_connection(\"CloudObjectStorage\")\ncos_auth_dict = json.loads(cos_connection_dict[\"credentials\"])\n\ncos_reader = utils.CloudObjectStorageReader(\n    bucket_name=cos_connection_dict[\"bucket\"],\n    credentials={\n        \"apikey\": cos_auth_dict[\"apikey\"],\n        \"service_instance_id\": cos_auth_dict[\"resource_instance_id\"],\n    },\n    hostname=f\"https://{cos_connection_dict['url']}\",\n    file_extractor=DEFAULT_READERS,\n)", "metadata": {"id": "a1c79063-510b-4fae-80ac-5d41347fc132"}, "outputs": [], "execution_count": 7}, {"cell_type": "markdown", "source": "### Count number of objects in the bucket", "metadata": {}}, {"cell_type": "code", "source": "import ibm_boto3\nfrom ibm_botocore.client import Config\nimport json\n\n# Get connection details\ncos_connection_dict = wslib.get_connection(\"CloudObjectStorage\")\ncos_auth_dict = json.loads(cos_connection_dict[\"credentials\"])\n\n# Initialize the COS client\ncos_client = ibm_boto3.client(\n    's3',\n    ibm_api_key_id=cos_auth_dict[\"apikey\"],\n    ibm_service_instance_id=cos_auth_dict[\"resource_instance_id\"],\n    config=Config(signature_version='oauth'),\n    endpoint_url=f\"https://{cos_connection_dict['url']}\"\n)\n\n# List objects in the specified bucket\nresponse = cos_client.list_objects_v2(Bucket=cos_connection_dict[\"bucket\"])\n\n# Count the number of objects in the bucket\nif 'Contents' in response:\n    num_objects = len(response['Contents'])\nelse:\n    num_objects = 0\n\nprint(f\"Number of objects in the bucket: {num_objects}\")", "metadata": {"id": "37414f94-71e9-4734-91b9-b83994a2d3e6"}, "outputs": [{"output_type": "stream", "text": "Number of objects in the bucket: 3\n", "name": "stdout"}], "execution_count": 12}, {"cell_type": "markdown", "source": "### Read files from Cloud Object Storage and convert them into LlamaIndex document objects", "metadata": {}}, {"cell_type": "code", "source": "documents = cos_reader.load_data(show_progress=True)\nprint(f\"Total documents: {len(documents)}\\nExample document:\\n{documents[0]}\")", "metadata": {"id": "d9619f3c-ddfb-42aa-a905-09c6d510dc14"}, "outputs": [{"output_type": "stream", "text": "Downloading files to temp dir: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  4.24it/s]\nLoading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:16<00:00,  5.63s/file]", "name": "stderr"}, {"output_type": "stream", "text": "Total documents: 400\nExample document:\nDoc ID: 976244db-b513-41eb-9475-ced6372c1a51\nText: 2021  Annual Report Let\u2019s create\n", "name": "stdout"}, {"output_type": "stream", "text": "\n", "name": "stderr"}], "execution_count": 8}, {"cell_type": "markdown", "source": "## Ingest chunked and formatted files into Watsonx Discovery \n\nNow that we have prepared the documents to be ingested into Watsonx Discovery, the next step is to actually ingest the files. To do this, we first need to connect to our Databases for Elasticsearch service using the connection in the project. Then, we perform the following steps to prepare the service:\n\n1. Try to download and deploy the ELSER model in the Databases for Elasticsearch service\n2. Create an index to store the documents\n3. Create a pipeline to specify the use of the ELSER model to create vector embeddings on document ingestion\n\nOnce these steps are done, we finally ingest the documents above into the index using the pipeline", "metadata": {}}, {"cell_type": "markdown", "source": "### Connect to Watsonx Discovery", "metadata": {}}, {"cell_type": "code", "source": "wxd_connection_dict = wslib.get_connection(\"WatsonxDiscovery\")\n\nes_client = Elasticsearch(\n    wxd_connection_dict[\"url\"],\n    basic_auth=(wxd_connection_dict[\"username\"], wxd_connection_dict[\"password\"]),\n    verify_certs=False,\n    request_timeout=3600,\n)\nasync_es_client = AsyncElasticsearch(\n    wxd_connection_dict[\"url\"],\n    basic_auth=(wxd_connection_dict[\"username\"], wxd_connection_dict[\"password\"]),\n    verify_certs=False,\n    request_timeout=3600,\n)\nes_client.info()", "metadata": {"id": "a846d373-78bb-4ccc-bc1b-8072ae2d160a"}, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "ObjectApiResponse({'name': 'm-1.09b2ea51-6fd9-4f27-a28c-f8cae949fb22.32327196cebf47ceba64c395a7c51f5a.bn2a2uid0up8mv7mv2ig.databases.appdomain.cloud', 'cluster_name': '09b2ea51-6fd9-4f27-a28c-f8cae949fb22', 'cluster_uuid': 'm_Obcka7SUSY-_USnpOmiA', 'version': {'number': '8.15.0', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': '1a77947f34deddb41af25e6f0ddb8e830159c179', 'build_date': '2024-08-05T10:05:34.233336849Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"}, "metadata": {}}], "execution_count": 13}, {"cell_type": "markdown", "source": "### Set up embedding model, index, and ingestion pipeline", "metadata": {}}, {"cell_type": "code", "source": "def download_model(\n    client: Elasticsearch, model_id: str, model_text_field: str = \"text\"\n) -> None:\n    \"\"\"\n    Downloads a trained model from elasticsearch.Elasticsearch if it doesn't already exist.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the trained model.\n        model_text_field (str): The name of the field the model looks for text to embed by default.\n\n    \"\"\"\n    try:\n        client.ml.get_trained_models(\n            model_id=model_id\n        )  # Throws error if model_id does not exist\n        print(f\"{model_id} already downloaded...\")\n    except elasticsearch.exceptions.NotFoundError:\n        model_schema = {\"input\": {\"field_names\": [model_text_field]}}\n        print(f\"Downloading {model_id}...\")\n        client.ml.put_trained_model(model_id=model_id, body=model_schema)\n        time.sleep(90)  # Wait for the model to be downloaded\n\n\ndef deploy_model(\n    client: Elasticsearch,\n    model_id: str,\n    deployment_id: Optional[str] = None,\n) -> None:\n    \"\"\"\n    Deploys a model through the Elasticsearch client.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the model to deploy.\n        deployment_id (str, optional): The ID to use for the deployment. Defaults to the model_id.\n\n    \"\"\"\n    if not deployment_id:\n        deployment_id = model_id\n\n    existing_deployments = (\n        client.ml.get_trained_models_stats(model_id=model_id)\n        .body[\"trained_model_stats\"][0]\n        .get(\"deployment_stats\")\n    )\n    if (\n        existing_deployments\n        and existing_deployments.get(\"deployment_id\") == deployment_id\n    ):\n        print(f\"{model_id} model deployment with the same name already exists.\")\n    else:\n        print(\n            f\"Creating {model_id} model deployment with deployment id {deployment_id}...\"\n        )\n        client.ml.start_trained_model_deployment(\n            model_id=model_id, deployment_id=deployment_id\n        )\n\n\ndef get_model_text_field(client: Elasticsearch, model_id: str) -> str:\n    \"\"\"\n    Retrieves the text field name from a trained model configuration.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the trained model.\n\n    Returns:\n        str: The name of the text field.\n    \"\"\"\n    response = client.ml.get_trained_models(model_id=model_id)\n    return response.body[\"trained_model_configs\"][0][\"input\"][\"field_names\"][0]\n\n\ndef create_index_with_default_pipeline(\n    client: Elasticsearch,\n    index_name: str,\n    index_config: dict,\n    pipeline_config: dict,\n    pipeline_name: str = \"default_pipeline\",\n) -> Tuple[elastic_transport.ObjectApiResponse, elastic_transport.ObjectApiResponse]:\n    \"\"\"\n    Creates an elasticsearch.Elasticsearch index with a default ingestion pipeline.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        index_name (str): The name of the index to be created.\n        index_config (dict): The configuration settings for the index.\n        pipeline_config (dict): The configuration settings for the ingestion pipeline.\n        pipeline_name (str, optional): The name of the ingestion pipeline. Defaults to \"default_pipeline\".\n\n    Returns:\n        tuple: A tuple containing the index creation response and the pipeline creation response.\n    \"\"\"\n    pipeline_name = pipeline_name or \"default_pipeline\"\n\n    if client.indices.exists(index=index_name):\n        print(f\"Deleting the existing index {index_name}...\")\n        client.indices.delete(index=index_name)\n\n    print(\"Creating the ingestion pipeline...\")\n    pipeline_response = client.ingest.put_pipeline(\n        id=pipeline_name, body=pipeline_config\n    )\n\n    print(\"Creating the index...\")\n    index_config[\"settings\"] = {\n        \"index.default_pipeline\": pipeline_name,\n    }\n    index_response = client.indices.create(index=index_name, body=index_config)\n    return index_response, pipeline_response", "metadata": {"id": "6af0ba3b-c8ec-443c-a87f-a9ebc41c878d"}, "outputs": [], "execution_count": 15}, {"cell_type": "code", "source": "download_model(es_client, EMBEDDING_MODEL_NAME, INDEX_TEXT_FIELD)\ndeploy_model(es_client, EMBEDDING_MODEL_NAME)\nembedding_model_text_field = get_model_text_field(es_client, EMBEDDING_MODEL_NAME)\n\nindex_config = {\n    \"mappings\": {\n        \"properties\": {\n            \"ml.tokens\": {\"type\": \"rank_features\"},\n            INDEX_TEXT_FIELD: {\"type\": \"text\"},\n        }\n    }\n}\n\npipeline_config = {\n    \"description\": \"Inference pipeline using ELSER embedding model\",\n    \"processors\": [\n        # Calculate embeddings\n        {\n            \"inference\": {\n                \"field_map\": {INDEX_TEXT_FIELD: embedding_model_text_field},\n                \"model_id\": EMBEDDING_MODEL_NAME,\n                \"target_field\": \"ml\",\n                \"inference_config\": {\"text_expansion\": {\"results_field\": \"tokens\"}},\n            }\n        },\n        # Give file_name and url their own fields for native Watsonx Assistant integration\n        {\"set\": {\"field\": \"file_name\", \"value\": \"{{metadata.file_name}}\"}},\n        {\"set\": {\"field\": \"url\", \"value\": \"{{metadata.url}}\"}},\n    ],\n    \"version\": 1,\n}", "metadata": {"id": "bd32187e-8a66-45fc-a93e-dcb8a04b765f"}, "outputs": [{"output_type": "stream", "text": ".elser_model_2_linux-x86_64 already downloaded...\n.elser_model_2_linux-x86_64 model deployment with the same name already exists.\n", "name": "stdout"}], "execution_count": 16}, {"cell_type": "code", "source": "create_index_with_default_pipeline(es_client, INDEX_NAME, index_config, pipeline_config)", "metadata": {"id": "4f7410a9-a472-4697-b77c-e13bd9570159"}, "outputs": [{"output_type": "stream", "text": "Creating the ingestion pipeline...\nCreating the index...\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "(ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'index-created-in-watson-studio'}),\n ObjectApiResponse({'acknowledged': True}))"}, "metadata": {}}], "execution_count": 17}, {"cell_type": "markdown", "source": "### Ingest the formatted documents", "metadata": {}}, {"cell_type": "code", "source": "Settings.embed_model = None\nSettings.llm = None\nSettings.node_parser = SentenceSplitter.from_defaults(\n    chunk_size=INGESTION_CHUNK_SIZE, chunk_overlap=INGESTION_CHUNK_OVERLAP\n)\n\nvector_store = ElasticsearchStore(\n    es_client=async_es_client, index_name=INDEX_NAME, text_field=INDEX_TEXT_FIELD\n)\ntry:\n    index = VectorStoreIndex.from_documents(\n        documents,\n        storage_context=StorageContext.from_defaults(vector_store=vector_store),\n        show_progress=True,\n    )\nexcept elasticsearch.helpers.BulkIndexError as e:\n    first_error = e.errors[0].get(\"index\", {}).get(\"error\", {})\n    print(f\"Bulk index error: {e}\")\n    print(\n        f\"Document ingestion to Elasticsearch failed with the following error: {first_error}\"\n    )", "metadata": {"id": "213dd95f-b3e2-4e98-a3ca-59b6626a8384"}, "outputs": [{"output_type": "stream", "text": "Embeddings have been explicitly disabled. Using MockEmbedding.\nLLM is explicitly disabled. Using MockLLM.\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Parsing nodes:   0%|          | 0/400 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "92f74b4e528d423c8cf505500c0fc098"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dbbe0156eb4748f89aa6d0f713f6749e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating embeddings:   0%|          | 0/1346 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e647f8cdd71443e78679ae39c0d7b8b3"}}, "metadata": {}}], "execution_count": 18}, {"cell_type": "markdown", "source": "### Test the ingested documents with a simple query", "metadata": {}}, {"cell_type": "code", "source": "query_engine = index.as_query_engine(\n    similarity_top_k=3,\n    vector_store_query_mode=\"sparse\",\n    vector_store_kwargs={\n        \"custom_query\": utils.create_sparse_vector_query_with_model(\n            EMBEDDING_MODEL_NAME\n        )\n    },\n)\nresponse = query_engine.query(\"What are some features of Watsonx.ai?\")\nfor source_node in response.source_nodes:\n    print(f\"{source_node.metadata['file_name']}:\\n {source_node.text}\\n\\n\")", "metadata": {"id": "4ccd26d9-0e3c-4c20-8751-b7a8b2700a19"}, "outputs": [{"output_type": "stream", "text": "IBM_Annual_Report_2023.pdf:\n IBM has built two powerful platforms \nto capitalize on the strong demand for both technologies: \nwatsonx for AI, and Red Hat OpenShift for hybrid cloud.\nWatsonx is our comprehensive AI and data platform, built to \ndeliver AI models and give our clients the ability to manage \nthe entire lifecycle of AI for business, including the training, \ntuning, deployment, and ongoing governance of those models. \nAs clients shift from experimenting with generative AI to \nbuilding and deploying it throughout their enterprises, we are \nfocused on practical and urgent business use cases, including \ncode modernization, customer service, and digital labor. \nFinancial institutions like Citi, Bradesco, and NatWest \nare using watsonx to help increase productivity, improve \ncode quality, and enhance customer experiences. Our \nenterprise-ready AI capabilities are being embedded into \nSAP solutions. EY launched EY.ai Workforce, a new solution \nthat will use watsonx Orchestrate to automate HR tasks and \nprocesses.\n\n\nIBM_Annual_Report_2023.pdf:\n These two strategic platforms complement one another. AI benefits from hybrid cloud through seamless access to data and \napplications across heterogeneous environments. Conversely, hybrid cloud is differentiated by AI\u2019s delivery of insights and \nautomation to streamline business, IT, and security processes. \nIn 2023, we launched watsonx as our AI platform for business. Built on Red Hat OpenShift, watsonx offers the power of state-of-\nthe-art IBM and open-source models for clients to run AI wherever they need it. The depth and breadth of our consulting expertise \nin generative AI can make a crucial difference in accelerating time-to-value for clients. Our offerings are uniquely differentiated, \ndelivering to clients an open and responsible approach to use multiple models, trusted AI governance solutions, and targeted use \ncases with proven value, including digital labor, customer service and software development. Watsonx and our Consulting \ncapabilities to deploy AI at-scale are a powerful combination distinctive to IBM. \nRed Hat OpenShift is the market-leading hybrid cloud application platform.\n\n\nIBM_Annual_Report_2023.pdf:\n As clients shift from experimenting with generative AI to \nbuilding and deploying it throughout their enterprises, we are \nfocused on practical and urgent business use cases, including \ncode modernization, customer service, and digital labor. \nFinancial institutions like Citi, Bradesco, and NatWest \nare using watsonx to help increase productivity, improve \ncode quality, and enhance customer experiences. Our \nenterprise-ready AI capabilities are being embedded into \nSAP solutions. EY launched EY.ai Workforce, a new solution \nthat will use watsonx Orchestrate to automate HR tasks and \nprocesses. Service partners such as NTT Data Business \nSolutions, Wipro, and TCS are launching watsonx Centers \nof Excellence to scale AI-powered client innovations. And \ngenerative AI from watsonx, combined with expertise from \nConsulting, is enhancing the digital experiences of the U.S. \nOpen, the Masters, Wimbledon, the GRAMMYs, and ESPN \nFantasy Football. \n2\n\n\n", "name": "stdout"}], "execution_count": 19}]}